# **Kafka**

## **Topic**

- 파티션은 늘릴 수는 있지만 다시 줄일 수는 없음
- 파티션 내의 데이터는 컨슈머가 컨슘한다해도 즉시 삭제되지 않음
- 동일 데이터에 대해선 아래의 경우 다른 컨슈머에 의해 두번 처리될 수 있음
  - 컨슈머 그룹이 다른 경우
  - auto.offset.reset = earliest 의 경우
- 삭제는 옵션에 따라 결정됨  
  log.retentions.ms: 최대 record 보존 시간  
  log.retention.byte: 최대 record 보존 크기 (byte)

<br>

## **Replication / broker**

- **파티션의 복제본을 저장하는곳**
  - replication 이 2라면 leader partion 1개 , follow partion 1개
  - replication 이 3이라면 leader partion - 1개 , follow partion - 2개

![kafka-image-1](https://user-images.githubusercontent.com/28802545/158009223-9c4b3c30-cbd7-4b0e-9928-1a837c91b473.PNG)

- **ack option**
  - **ack 0**
    - producer 는 leader partion 에 data 를 전송하고 응답값을 받지 않음
    - 그렇기 때문에 leader partion에 데이터가 정상적으로 전송되었는지 나머지 partion에 정상적으로 복제되었는지 보장할 수 없음
    - 속도는 빠르지만 데이터 유실 가능성이 있음
  - **ack 1**
    - producer 는 leader partion 에 data 를 전송하고 응답값을 받음
    - 다만 나머지 partion에 복제되었는지는 알 수 없음
    - 데이터 유실 가능성 존재
  - **ack all**
    - producer 는 leader partion 에 data 를 전송하고 응답값을 받음
    - 나머지 partion에도 data가 저장되는 것까지 확인함
    - 데이터 유실은 없는대신 0,1 에 비해 확인과정이 많기때문에 속도가 느림

<br>

- **파티셔너(Partitioner): 프로듀서가 data를 보내면 파티셔너를 통해 브로커로 데이터가 전달됨**

  - **메시지 키가 있는경우**

    - 메시지키를 가진 레코드는 파티셔너에 의해 특정한 hash 값이 생성됨, 이 hash값을 기준으로 어느 파티션에 들어갈지 결정됨
    - 동일한 키를 가진 레코드는 동일한 파티션으로 순차적으로 들어가게됨 (queue 처럼 동작)

  - **메시지 키가 없는경우**
    - `round-robin` 방식으로 파티션에 들어가게됨
    - 그런데 일반적인 `round-robin` 과는 약간 다르게 `UniformStickyPartitioner` 이라는 파티셔너가 프로듀서에서 배치로 모을수 있는 최대한의 레코드를 모아 파티션으로 데이터를 전송 (파티셔너를 따로 설정하지 않은 경우 default 가 `UniformStickyPartitioner`)
    - custom 파티셔너를 사용할 수 있게 파티셔너 interface 제공

- **lag**

  - producer 가 data 를 넣는 속도가 consumer 가 data 를 가져가는 속도보다 빨라 둘의 offset 차이가 발생하는것, 둘의 offset 차이를 lag 이라 한다. (producer offset: 토픽의 가장 최근 offset)

    ![kafka-image-2](https://user-images.githubusercontent.com/28802545/158011764-7bab74ee-7553-4641-b071-bc592f643bb8.PNG)

  - partion이 여러개인 경우 lag 도 여러개가 존재할 수 있음
  - lag 중 가장 높은 숫자의 lag 을 `redords-lag-max` 라고 함
  - lag 모니터링 도구로는 LinkedIn 에서 만든 [Burrow](https://github.com/linkedin/Burrow) 가 있음

- **메시지브로커와 이벤트브로커**

  - 메시지브로커는 메시지브로커 역할만 가능  
    이벤트 브로커는 이벤트,메시지 브로커 둘의 역할 모두 수행 가능
  - 메시지브로커 특징
    - 메시지를 받아 적절히 처리하고 나면 즉시 or 짧은 시간 내에 데이터가 삭제되는 구조
    - ex) redis queue, rabbitmq
  - 이벤트브로커 특징
    - 이벤트 or 메시지라고 불리는 레코드를 인덱스를 통해 개별 엑세스를 관리
    - 필요한 시간동안 이벤트를 보존 가능
    - 이벤트를 저장함으로서 얻는 이점은 장애 발생시 장애가 일어난 지점부터 재처리 가능 / 많은 양의 stream data를 효과적으로 처리 가능
    - ex) kafaka, aws kinesis, aws sqs

- **컨슈머(Consumer)**

  - 파티션과 컨슈머는 N:1 의 관계를 가질 수 있다.

    - 파티션2개 컨슈머1개이면 1개의 컨슈머가 2개의 파티션을 모두 컨슘한다.
    - 파티션2개 컨슈머2개이면 각각의 파티션을 각각의 컨슈머가 컨슘한다.
    - 파티션2개 컨슈머3개이면 컨슈머 하나는 아무일도 하지않고 놀게된다.
    - 즉, 컨슈머의 개수는 파티션의 개수보다 같거나 작아야 한다.

<br>

- 하나의 토픽은 여러개의 컨슈머 그룹으로부터 컨슘될 수 있다.
  - 각각의 컨슈머 그룹은 독립적이어서 서로에게 영향을 주지 않는다.
- 아래와 같이 컨슈머된 데이터를 하나의 컨슈머그룹에선 elastic search 에 적재하고 다른 컨슈머 그룹에선 hadoop 에 적재하는식의 활용이 가능하다.

![kafka-image-3](https://user-images.githubusercontent.com/28802545/158048767-e36120cf-3996-4243-b79c-20800712a2fd.PNG)

- **Kafka Streams**

  - Kafka 와 완벽 호환
  - 데이터 유실혹은 중복처리가 되지않고 한번만 처리되는것을 보장한다
  - 스케줄링 도구가 필요 없음
    이벤트 처리 기능(Streams DSL , Processor API) 제공
  - 자체 로컬 상태 저장소를 사용
    - 상태 기반 처리를 돕기 위해 rocksDB 를 로컬에서 사용하여 상태를 저장

- **Streams vs Consumer**  
  `Consumer`와의 어떤 차이가 있을까

  - `Consumer`는 메시지를 받아 단순히 처리하고 끝낸다
  - `Kafka Streams`는 메시지를 받아 연속된 처리를 하는 pipeline 구성 가능  
    ex) 메시지의 민감 데이터 마스킹,  
    1분 간격으로 메시지 특정 이벤트 감지

### Kafka Zero-Copy

- kafka는 메시지 전송시 `zero-copy` 방식을 사용  
  (기존에는 read하고 send하기 떄문에 메모리 복사, 컨텍스트 스위칭이 있어 비용이 비쌌음)

기존 방식

```
1. 클라이언트가 서버에게 정적 파일을 요청
2. 서버의 웹 어플리케이션이 요청을 받음
3. 웹 어플리케이션(유저 영역)이 디스크에서 파일 데이터를 읽기 위해서 커널(커널 영역)이 파일 데이터를 읽도록 요청
4. 파일을 다 읽은 후 커널(커널 영역)이 다시 웹 어플리케이션(유저 영역)으로 데이터를 반환
5. 하지만 웹 어플리케이션(유저 영역)은 클라이언트에 반환하기 위해서 소켓에 파일 데이터를 넣어야 하므로 다시 커널(커널 영역)에 요청
6. 커널(커널 영역)은 소켓을 처리
```

위 4,5번 항목의 작업에서 불필요한 `Context Switching`이 많이 발생  
이에 반해 `zero-copy`는 커널영역에서 데이터를 읽은 후 웹 어플리케이션 영역으로 돌아가지 않고 바로 소켓에 데이터를 담아 전달(말 그대로 무복사)

### Kafka 핵심 개념

- 분산 처리
  - 간편하게 브로커의 수를 확장할 수 있음
- 페이지 캐시
  - OS의 성능을 높이기 위해 캐시를 사용함, 디스크I/O에 대한 접근이 줄어 성능을 높일 수 있음
- 배치 전송 처리
  - 로그성 시스템은 실시간으로 전송할 필요가 없으니 배치로 전송할때 효율적임
- 압축 전송
  - 압축은 네트워크 대역폭이나 회선 비용을 줄이는데 효과적
  - 앞선 배치전송과 결합해 사용하면 더 높은 효율이 있음
- 토픽, 파티션, 오프셋
- 고가용성 (High Availability)
  - 무조건 팔로워수가 많다고 좋은것이 아니다  
    팔로워 수 많큼 브로커의 디스크 공간도 소비되므로 이상적인 리플리케이션 팩터 수를 유지해야 한다.  
    일반적으로 3으로 구성하도록 권장

### 리플리케이션 (Replication)

- 리더와 팔로워 사이에는 ack 통신이 없다  
  (ack - 여기선 팔로워들이 리더의 데이터를 리플리케이션에 성공했는지 여부를 확인하는것이라 보면 된다)  
  여타 메시징 시스템에선 ack가 있지만 카프카에선 ack를 제거하고 성능 을 높였다.

#### 그렇다면 kafka에서는 어떻게 ack를 주고받지 않고도 팔로워들이 안정적으로 리플리케이션 동작을 하는지 알 수 있을까?

팔로워들은 리더에게 n번 offset에 대해 리플리케이션을 요청한다.  
이때 팔로워들은 자신이 n번의 offset에 리플리케이션에 성공하지 못했다면 다음 offset을 요청하지 않고 다시 똑같은 번호의 offset으로 리플리케이션 요청을 보낸다.
리더는 팔로워들이 보내는 리플리케이션 offset을 보고 팔로워들이 어느 offset까지 리플리케이션을 성공했는지 인지할 수 있다.  
따라서 리플리케이션 요청을 받은 리더는 응답에 이전 offset이 커밋되었다는 내용도 같이 전달한다.

#### 카프카에서 리더와 팔로워들의 리플리케이션 동작방식

리더가 메시지를 push하는 방식이 아니라 팔로워들이 pull 하는 방식으로 동작  
이는 리플리케이션 동작에서 리더의 부하를 줄여준다

<br>

#### **리더에포크**

팔로워가 장애가 있다가 다시 복귀되었을때 `리더에포크`가 없다면 자신의 `하이워터마크`보다 높은 메시지는 삭제한다.  
이때 다시 리더에게 리플리케이션요청을 하지만 만약 리더가 장애로인해 죽었다면 팔로워가 리더가 된다. 하지만 그렇게 되면 메시지가 손실될 수 있다.  
만약 `리더에포크`를 활용하게 되면 팔로워가 장애 이후 다시 복구되었을때 자신의 `하이워터마크`보다 높은 메시지가 있다면 삭제하는것이 아니라 리더에게 `리더에포크` 요청을 보낸다.  
리더는 팔로워에게 `n번 offset까지` 라고 응답을 보낸다. 팔로워는 자신의 `하이워터마크` 보다 높은 메시지를 삭제하지 않고 리더의 응답을 확인한 후 자신의 `하이워터마크`를 상향조정한다.

<br>

또 다른 케이스로 브로커에 장애가 발생해 리더와 팔로워 서버가 죽었다 다시 살아나며 팔로워가 먼저 살아나 리더가 되고 구 리더는 팔로워가 된 경우이다.  
이 경우엔 새로운 리더는 자신이 팔로워일때의 `하이워터마크`와 현재의 `하이워터마크`를 알고있다.  
구 리더인 팔로워는 리더에게 `리더에포크` 요청을 보낸다.  
그리고 뉴 리더는 둘의 `하이워터마크` 및 메시지를 비교해 n offset까지 유효하다는 응답을 하고 불일치하는 메시지는 삭제한다.  
이후 팔로워(구 리더)는 다음 메시지에 대해 리플리케이션 요청을 한다..

<br>

#### __컨트롤러__

리더 선출을 담당. 카프카 클러스터 중 하나의 브로커가 컨트롤러 역할을 하게 된다.  
파티션의 __ISR(In Sync Replication)__ 리스트 중 리더를 선출한다.  
리더를 선출하기 위한 __ISR(In Sync Replication)__ 리스트 정보는 고가용성 보장을 위해 주키퍼에 저장되어 있다.  
컨트롤러는 브로커가 실패하는 것을 모니터링하다가 실패가 감지되면 즉시 __ISR(In Sync Replication)__리스트 중 하나를 새로운 파티션 리더로 선출한다.