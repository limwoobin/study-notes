# **Kafka**

## **Topic**

- 파티션은 늘릴 수는 있지만 다시 줄일 수는 없음
- 파티션 내의 데이터는 컨슈머가 컨슘한다해도 즉시 삭제되지 않음
- 동일 데이터에 대해선 아래의 경우 다른 컨슈머에 의해 두번 처리될 수 있음
  - 컨슈머 그룹이 다른 경우
  - auto.offset.reset = earliest 의 경우
- 삭제는 옵션에 따라 결정됨  
  log.retentions.ms: 최대 record 보존 시간  
  log.retention.byte: 최대 record 보존 크기 (byte)

<br>

## **Replication / broker**

- **파티션의 복제본을 저장하는곳**
  - replication 이 2라면 leader partion 1개 , follow partion 1개
  - replication 이 3이라면 leader partion - 1개 , follow partion - 2개

![kafka-image-1](https://user-images.githubusercontent.com/28802545/158009223-9c4b3c30-cbd7-4b0e-9928-1a837c91b473.PNG)

- **ack option**
  - **ack 0**
    - producer 는 leader partion 에 data 를 전송하고 응답값을 받지 않음
    - 그렇기 때문에 leader partion에 데이터가 정상적으로 전송되었는지 나머지 partion에 정상적으로 복제되었는지 보장할 수 없음
    - 속도는 빠르지만 데이터 유실 가능성이 있음
  - **ack 1**
    - producer 는 leader partion 에 data 를 전송하고 응답값을 받음
    - 다만 나머지 partion에 복제되었는지는 알 수 없음
    - 데이터 유실 가능성 존재
  - **ack all**
    - producer 는 leader partion 에 data 를 전송하고 응답값을 받음
    - 나머지 partion에도 data가 저장되는 것까지 확인함
    - 데이터 유실은 없는대신 0,1 에 비해 확인과정이 많기때문에 속도가 느림

<br>

## **카프카 브로커(Kafka Broker)**

- 실행된 Kafka Application 서버를 뜻함
- 카프카 클라이언트와 데이터를 주고받기 위해 사용되는 주체
- 데이터를 분산 저장하여 장애가 발생하더라도 안전하게 사용할 수 있도록 도와줌
- 브로커 내부에 여러 토픽들이 생성될 수 있고 이러한 토픽들에 의해 생성된 파티션들이 보관하는 데이터에 대해 분산 저장을 해주거나 장애 발생 시, 안전하게 데이터를 사용할 수 있도록 도와주는 역할

1) Controller: 클러스터의 여러 브로커 중 하나의 브로커가 컨트롤러 역할을 하게 됨  
컨트롤러는 클러스터 내의 브로커가 장애로 인해 사용할 수 없는 경우, 장애가 발생한 브로커의 토픽에 있는 리더 파티션을  
같은 클러스터 내의 정상 동작하는 다른 브로커에게 토픽의 리더 파티션 지위를 재분배하는 역할을 함

2) Coordinator
클러스터의 여러 브로커 중 하나의 브로커가 코디네이터 역할을 함  
컨슈머 그룹의 상태를 체크하여 컨슈머 그룹 내의 컨슈머가 장애가 발생해 파티션의 데이터를 consume할 수 없는 경우,  
장애가 발생한 컨슈머가 컨슘하던 파티션을 다른 컨슈머에게 매핑해줌  
이 때, 파티션을 컨슈머에게 재할당 하는 과정을 리밸런싱(rebalancing)이라 함

3) Cluster
클러스터는 브로커로 이루어진 집합체를 말함

## **주키퍼(Zookeeper)**

- 분산 코디네이션 서비스를 제공하는 오픈소스 프로젝트
- 각 어플리케이션(Broker)의 정보를 중앙 집중화하고, 구성관리, 동기화 등의 서비스 제공
- 카프카를 띄우기 위해서는 주키퍼가 반드시 실행되어야 함
- MetaData(브로커id, 컨트롤러id, 여러 환경설정 등등) 저장
- 상태 정보들은 주키퍼의 지노드(znode)라는 곳에 Key-Value 형태로 저장

1) Zookeeper Ensemble  
주키퍼의 클러스터를 주키퍼 앙상블(Zookeeper Ensemble)이라 부름

<br>

- **파티셔너(Partitioner): 프로듀서가 data를 보내면 파티셔너를 통해 브로커로 데이터가 전달됨**

  - **메시지 키가 있는경우**

    - 메시지키를 가진 레코드는 파티셔너에 의해 특정한 hash 값이 생성됨, 이 hash값을 기준으로 어느 파티션에 들어갈지 결정됨
    - 동일한 키를 가진 레코드는 동일한 파티션으로 순차적으로 들어가게됨 (queue 처럼 동작)

  - **메시지 키가 없는경우**
    - `round-robin` 방식으로 파티션에 들어가게됨
    - 그런데 일반적인 `round-robin` 과는 약간 다르게 `UniformStickyPartitioner` 이라는 파티셔너가 프로듀서에서 배치로 모을수 있는 최대한의 레코드를 모아 파티션으로 데이터를 전송 (파티셔너를 따로 설정하지 않은 경우 default 가 `UniformStickyPartitioner`)
    - custom 파티셔너를 사용할 수 있게 파티셔너 interface 제공

- **lag**

  - producer 가 data 를 넣는 속도가 consumer 가 data 를 가져가는 속도보다 빨라 둘의 offset 차이가 발생하는것, 둘의 offset 차이를 lag 이라 한다. (producer offset: 토픽의 가장 최근 offset)

    ![kafka-image-2](https://user-images.githubusercontent.com/28802545/158011764-7bab74ee-7553-4641-b071-bc592f643bb8.PNG)

  - partion이 여러개인 경우 lag 도 여러개가 존재할 수 있음
  - lag 중 가장 높은 숫자의 lag 을 `redords-lag-max` 라고 함
  - lag 모니터링 도구로는 LinkedIn 에서 만든 [Burrow](https://github.com/linkedin/Burrow) 가 있음

- **메시지브로커와 이벤트브로커**

  - 메시지브로커는 메시지브로커 역할만 가능  
    이벤트 브로커는 이벤트,메시지 브로커 둘의 역할 모두 수행 가능
  - 메시지브로커 특징
    - 메시지를 받아 적절히 처리하고 나면 즉시 or 짧은 시간 내에 데이터가 삭제되는 구조
    - ex) redis queue, rabbitmq
  - 이벤트브로커 특징
    - 이벤트 or 메시지라고 불리는 레코드를 인덱스를 통해 개별 엑세스를 관리
    - 필요한 시간동안 이벤트를 보존 가능
    - 이벤트를 저장함으로서 얻는 이점은 장애 발생시 장애가 일어난 지점부터 재처리 가능 / 많은 양의 stream data를 효과적으로 처리 가능
    - ex) kafaka, aws kinesis, aws sqs

- **컨슈머(Consumer)**

  - 파티션과 컨슈머는 N:1 의 관계를 가질 수 있다.

    - 파티션2개 컨슈머1개이면 1개의 컨슈머가 2개의 파티션을 모두 컨슘한다.
    - 파티션2개 컨슈머2개이면 각각의 파티션을 각각의 컨슈머가 컨슘한다.
    - 파티션2개 컨슈머3개이면 컨슈머 하나는 아무일도 하지않고 놀게된다.
    - 즉, 컨슈머의 개수는 파티션의 개수보다 같거나 작아야 한다.

<br>

- 하나의 토픽은 여러개의 컨슈머 그룹으로부터 컨슘될 수 있다.
  - 각각의 컨슈머 그룹은 독립적이어서 서로에게 영향을 주지 않는다.
- 아래와 같이 컨슈머된 데이터를 하나의 컨슈머그룹에선 elastic search 에 적재하고 다른 컨슈머 그룹에선 hadoop 에 적재하는식의 활용이 가능하다.

![kafka-image-3](https://user-images.githubusercontent.com/28802545/158048767-e36120cf-3996-4243-b79c-20800712a2fd.PNG)

- **Kafka Streams**

  - Kafka 와 완벽 호환
  - 데이터 유실혹은 중복처리가 되지않고 한번만 처리되는것을 보장한다
  - 스케줄링 도구가 필요 없음
    이벤트 처리 기능(Streams DSL , Processor API) 제공
  - 자체 로컬 상태 저장소를 사용
    - 상태 기반 처리를 돕기 위해 rocksDB 를 로컬에서 사용하여 상태를 저장

- **Streams vs Consumer**  
  `Consumer`와의 어떤 차이가 있을까

  - `Consumer`는 메시지를 받아 단순히 처리하고 끝낸다
  - `Kafka Streams`는 메시지를 받아 연속된 처리를 하는 pipeline 구성 가능  
    ex) 메시지의 민감 데이터 마스킹,  
    1분 간격으로 메시지 특정 이벤트 감지

### Kafka Zero-Copy

- kafka는 메시지 전송시 `zero-copy` 방식을 사용  
  (기존에는 read하고 send하기 떄문에 메모리 복사, 컨텍스트 스위칭이 있어 비용이 비쌌음)

기존 방식

```
1. 클라이언트가 서버에게 정적 파일을 요청
2. 서버의 웹 어플리케이션이 요청을 받음
3. 웹 어플리케이션(유저 영역)이 디스크에서 파일 데이터를 읽기 위해서 커널(커널 영역)이 파일 데이터를 읽도록 요청
4. 파일을 다 읽은 후 커널(커널 영역)이 다시 웹 어플리케이션(유저 영역)으로 데이터를 반환
5. 하지만 웹 어플리케이션(유저 영역)은 클라이언트에 반환하기 위해서 소켓에 파일 데이터를 넣어야 하므로 다시 커널(커널 영역)에 요청
6. 커널(커널 영역)은 소켓을 처리
```

위 4,5번 항목의 작업에서 불필요한 `Context Switching`이 많이 발생  
이에 반해 `zero-copy`는 커널영역에서 데이터를 읽은 후 웹 어플리케이션 영역으로 돌아가지 않고 바로 소켓에 데이터를 담아 전달(말 그대로 무복사)

### Kafka 핵심 개념

- 분산 처리
  - 간편하게 브로커의 수를 확장할 수 있음
- 페이지 캐시
  - OS의 성능을 높이기 위해 캐시를 사용함, 디스크I/O에 대한 접근이 줄어 성능을 높일 수 있음
- 배치 전송 처리
  - 로그성 시스템은 실시간으로 전송할 필요가 없으니 배치로 전송할때 효율적임
- 압축 전송
  - 압축은 네트워크 대역폭이나 회선 비용을 줄이는데 효과적
  - 앞선 배치전송과 결합해 사용하면 더 높은 효율이 있음
- 토픽, 파티션, 오프셋
- 고가용성 (High Availability)
  - 무조건 팔로워수가 많다고 좋은것이 아니다  
    팔로워 수 많큼 브로커의 디스크 공간도 소비되므로 이상적인 리플리케이션 팩터 수를 유지해야 한다.  
    일반적으로 3으로 구성하도록 권장

### 리플리케이션 (Replication)

- 리더와 팔로워 사이에는 ack 통신이 없다  
  (ack - 여기선 팔로워들이 리더의 데이터를 리플리케이션에 성공했는지 여부를 확인하는것이라 보면 된다)  
  여타 메시징 시스템에선 ack가 있지만 카프카에선 ack를 제거하고 성능 을 높였다.

#### 그렇다면 kafka에서는 어떻게 ack를 주고받지 않고도 팔로워들이 안정적으로 리플리케이션 동작을 하는지 알 수 있을까?

팔로워들은 리더에게 n번 offset에 대해 리플리케이션을 요청한다.  
이때 팔로워들은 자신이 n번의 offset에 리플리케이션에 성공하지 못했다면 다음 offset을 요청하지 않고 다시 똑같은 번호의 offset으로 리플리케이션 요청을 보낸다.
리더는 팔로워들이 보내는 리플리케이션 offset을 보고 팔로워들이 어느 offset까지 리플리케이션을 성공했는지 인지할 수 있다.  
따라서 리플리케이션 요청을 받은 리더는 응답에 이전 offset이 커밋되었다는 내용도 같이 전달한다.

#### 카프카에서 리더와 팔로워들의 리플리케이션 동작방식

리더가 메시지를 push하는 방식이 아니라 팔로워들이 pull 하는 방식으로 동작  
이는 리플리케이션 동작에서 리더의 부하를 줄여준다

<br>

#### **리더에포크**

팔로워가 장애가 있다가 다시 복귀되었을때 `리더에포크`가 없다면 자신의 `하이워터마크`보다 높은 메시지는 삭제한다.  
이때 다시 리더에게 리플리케이션요청을 하지만 만약 리더가 장애로인해 죽었다면 팔로워가 리더가 된다. 하지만 그렇게 되면 메시지가 손실될 수 있다.  
만약 `리더에포크`를 활용하게 되면 팔로워가 장애 이후 다시 복구되었을때 자신의 `하이워터마크`보다 높은 메시지가 있다면 삭제하는것이 아니라 리더에게 `리더에포크` 요청을 보낸다.  
리더는 팔로워에게 `n번 offset까지` 라고 응답을 보낸다. 팔로워는 자신의 `하이워터마크` 보다 높은 메시지를 삭제하지 않고 리더의 응답을 확인한 후 자신의 `하이워터마크`를 상향조정한다.

<br>

또 다른 케이스로 브로커에 장애가 발생해 리더와 팔로워 서버가 죽었다 다시 살아나며 팔로워가 먼저 살아나 리더가 되고 구 리더는 팔로워가 된 경우이다.  
이 경우엔 새로운 리더는 자신이 팔로워일때의 `하이워터마크`와 현재의 `하이워터마크`를 알고있다.  
구 리더인 팔로워는 리더에게 `리더에포크` 요청을 보낸다.  
그리고 뉴 리더는 둘의 `하이워터마크` 및 메시지를 비교해 n offset까지 유효하다는 응답을 하고 불일치하는 메시지는 삭제한다.  
이후 팔로워(구 리더)는 다음 메시지에 대해 리플리케이션 요청을 한다..

<br>

#### **컨트롤러**

리더 선출을 담당. 카프카 클러스터 중 하나의 브로커가 컨트롤러 역할을 하게 된다.  
파티션의 **ISR(In Sync Replication)** 리스트 중 리더를 선출한다.  
리더를 선출하기 위한 **ISR(In Sync Replication)** 리스트 정보는 고가용성 보장을 위해 주키퍼에 저장되어 있다.  
컨트롤러는 브로커가 실패하는 것을 모니터링하다가 실패가 감지되면 즉시 **ISR(In Sync Replication)**리스트 중 하나를 새로운 파티션 리더로 선출한다.

<br>

#### **Kafka Log**

<hr>
<br>

### **파티셔너**

토픽은 성능 향상을 위한 병렬처리가 가능하도록 하기 위해 최소 하나 또는 둘 이상의 파티션으로 구성됨  
프로듀서는 토픽으로 메시지를 보낼때 어느 파티션으로 메시지를 보내야 할지 결정해야 하는데, 이때 사용하는것이 **파티셔너(Partitioner)**  
파티션을 결정하는 방식은 기본적으로 메시지의 키를 해시(hash)처리해 파티션을 구함,  
따라서 메시지의 키값이 동일하면 해당 메시지는 모두 같은 파티션으로 전송됨  
만약 많은 양의 메시지가 유입되면 카프카는 처리량을 높이기 위해 파티션을 늘릴 수 있음, 이때 파티션 수가 변경되면 메시지의 키와 매핑된 해시테이블도 변경됨  
따라서 이런 경우에는 동일한 메시지의 키를 전송하더라도 다른 파티션으로 전송될 수 있음

#### **라운드 로빈(Round Robin)** (중요)

프로듀서의 메시지 중 레코드 키값은 필수값이 아님, 관리자는 별도의 키값을 지정하지 않고 메시지를 전송할 수 있음  
만약 키 값이 지정되지 않았다면 키 값은 null이 되고, 이 경우엔 **라운드 로빈(Round Robin)** 전략을 이용해 프로듀서는 목적지 토픽의 파티션으로 메시지를 전송함  
하지만 레코드들은 배치 처리를 위해 프로듀서의 버퍼 메모리 영역에서 잠시 대기한 후 카프카로 전송됨  
이때 배치 처리를 위해 잠시 메시지를이 대기하는 과정에서 **라운드 로빈(Round Robin)**전략은 효율을 떨어트릴 수 있음 (배치 전송을 위한 최소 레코드 수를 충족하지 못했기 때문)

이를 보완하기 위해 나온게 **스티키 파티셔닝(sticky partitioning)**

<br>

#### **스티키 파티셔닝(sticky partitioning)**

라운드 로빈에선 배치전송을 위한 필요 레코드 수를 채우지 못해 카프카로 배치전송을 하지 못했던 것과 달리,  
스티키 파티셔닝은 하나의 파티션에 레코드 수를 먼저 채워 카프카로 빠르게 배치 전송하는 전략

<br>

#### **프로듀서의 배치**

카프카에서는 토픽의 처리량을 높이기 위해 토픽을 파티션으로 나눠 처리하며,  
카프카 클라이언트인 프로듀서는 처리량을 높기이 위해 **배치 전송**을 권장함

- buffer.memory: 카프카로 메시지를 전송하기 위해 담아두는 프로듀서의 버퍼 메모리 옵션
- batch.size: 배치 전송을 위해 메시지(레코드)들을 묶는 단위를 설정하는 배치 크기 옵션
- linger.ms: 배치 전송을 위해 버퍼 메모리에서 대기하는 메시지들의 최대 대기시간을 설정하는 옵션 (단위는 ms이며 기본값은 0)  
  즉, 기본값이 0이면 배치전송을 위해 대기하지 않고 즉시 전송됨

카프카를 사용하는 목적에 따라 배치를 이용해 처리량을 높일지, 아니면 지연없는 전송을 해야 할지 선택해야함  
대량의 메시지를 처리할 때 처리량을 높여야 하는 경우라면 사용자는 효율적인 전송을 위해 배치 전송을 해야함, 반대로 지연 없는 전송이 목적이라면 프로듀서의 배치 전송 관련 설정을 제거해야함  
즉, 처리량을 높이려면 batch.size, linger.ms의 값을 크게 설정  
지연없는 전송이 목적이라면 batch.size, linger.ms의 값을 작게 설정

사용자가 프로듀서의 높은 처리량을 목푤 배치 전송을 설정하는 경우 주의해야할 사항  
-> 바로 버퍼 메모리의 크기가 충분히 커야한다는 점  
즉, buffer.memory의 크기는 batch.size보다 커야함  
예를들어 토픽A가 3개의 파티션을 갖고 있고 batch.size는 16kb라고 가정한다면 프로듀서의 buffer.memory의 크기는 최소 16kb \* 3(48kb)가 되어야 함

배치 전송과 압축 기능을 같이 사용하면 프로듀서는 메시지를 더 효율적으로 전송할 수 있음  
클라이언트를 포함해 카프카에서는 gzip, snappy, lz4, zstd등의 압축 포맷을 지원함  
각 압축포맷마다 특징이 있는데 메시지들의 높은 압축률을 선호한다면 gzip, zstd를 선택하는것이 좋고, 낮은 지연시간을 선호하면 lz, snappy를 선택하는것이 좋음

<br>

#### __중복없는 전송__

#### __정확히 한 번 전송__


<br>

----------------------------------------------

# __Kafka Streams...__

- 카프카 스트림즈는 카프카에 저장된 데이터를 처리하고 분석하기 위한 라이브러리.  
- 컨슈머를 사용해 데이터를 처리하는것보다 더 안전하고 빠르고 다양한 기술을 사용할 수 있음  
- 토픽에 적재된 데이터를 실시간으로 변환하여 다른 토픽에 적재가능


## 특징

## 장/단점

## 사용법

----------------------------------------------

## 카프카 리밸런싱

- `max.poll.records`: poll() 시에 읽어올 최대 record 수 (default 500)
- `max.poll.interval.ms`: 컨슈머 poll() 대기시간, 해당 시간동안 poll() 이 일어나지 않을 경우 리밸런싱 발생 (기본값 5분)

### 카프카 리밸런싱이 일어나는 경우

1. 컨슈머의 생성 / 삭제
컨슈머가 생성/삭제되는 가장 일반적인 경우는 배포 할 때.  
배포 과정에서 기존 어플리케이션이 종료되고, 새 어플리케이션이 다시 동작하게 된다. 이때 리밸런싱은 최소 두 번 일어나게 된다.  
기존 컨슈머가 삭제 될 때, 새로운 컨슈머가 생성될 때

2. 시간안에 poll 요청 실패
컨슈머는 `max.poll.records` 설정의 개수만큼 메시지를 처리한 뒤 Poll 요청을 보내게 됨.  
하지만 메시지들의 처리가 늦어져 `max.poll.interval.ms` 설정 시간을 넘기게 된다면 컨슈머에 문제가 있다고 판단해 리밸런싱이 일어남.

3. 컨슈머 문제 발생
컨슈머가 일정 시간 동안 하트비트를 보내지 못하면, 세션이 종료되고 컨슈머 그룹에서 제외됨.  
이때, 리밸런싱이 진행됨

### 카프카 리밸런싱의 리스크는 ?
1. 컨슈머 처리 중단(파티션 읽기 작업 중단)
리배런싱이 완료되기 전까지는 컨슈머는 동작하지 않음.

2. 메시지 중복 컨슈밍
컨슈머에서 10개의 메시지중 5개까지 처리하다가 `max.poll.interval.ms` 시간이 지나 offset 커밋에 실패.  
이때, 다른 컨슈머에서 메시지를 처리하게 되면 중복 메시지가 처리됨

### 리밸런싱 리스크를 해결하기 위해서는 ?
A) `max.poll.records` 조정하기. 
`max.poll.records` 를 줄이면 다음과 같은 장점이 있음 (default = 500)

1. 리밸런싱 시간 단축
`max.poll.records` 값이 작을수록 Poll 요청을 빠르게 보낼 수 있음 (왜냐면 처리해야 할 데이터가 적기 때문). 
즉, `max.poll.records` 값을 작게 할수록 리밸런싱 작업이 빠르게 완료됨
2. Poll 지연에 의한 리밸런싱 발생 가능성 감소
`max.poll.interval.ms` 안에 Poll 요청을 보내지 않으면 리밸런싱이 일어남.
하지만 `max.poll.records` 값이 작을수록 Poll 요청을 빠르게 보내게 되어 리밸런싱이 발생할 가능성이 줄어듬
3. 메시지 중복 컨슈밍 가능성 감소
`max.poll.records` 이 적으면 Poll 요청의 빈도가 늘게되어, 중복 컨슈밍의 가능성이 감소하게 됨


----------------------------------------------

## 카프카 옵션

- session.timeout.ms (default 10000ms (10초))  
컨슈머와 브로커사이의 session timeout 시간.
컨슈머가 살아있는것으로 판단하는 시간으로 이 시간이 지나면 해당 컨슈머는 종료되거나 장애가 발생한것으로 판단하고 컨슈머 그룹은 리밸런스를 시도한다.
이 옵션은 heartbeat 없이 얼마나 오랫동안 컨슈머가 있을 수 있는지를 제어하며 heartbeat.interval.ms와 밀접한 관련이 있어서 일반적으로 두 속성이 함께 수정된다.
- heartbeat.interval.ms (default 3000ms (3초))
컨슈머가 얼마나 자주 heartbeat을 보낼지 조정한다. session.timeout.ms보다 작아야 하며 일반적으로 1/3로 설정  
- max.poll.interval.ms (default 300000ms (5분))
컨슈머가 polling하고 commit 할때까지의 대기시간. 컨슈머가 살아있는지를 체크하기 위해 hearbeat를 주기적으로 보내는데, 계속해서 heartbeat만 보내고 실제로 메시지를 가져가지 않는 경우가 있을 수 있다.
이러한 경우에 컨슈머가 무한정 해당 파티션을 점유할 수 없도록 주기적으로 poll을 호출하지 않으면 장애라고 판단하고 컨슈머 그룹에서 제외시키도록 하는 옵션이다.  
- max.poll.records (default 500개)
컨슈머가 최대로 가져 갈 수있는 갯수. 이 옵션으로 polling loop에서 데이터 양을 조정 할 수 있다. 
- enable.auto.commit (default true)
백그라운드로 주기적으로 offset을 commit
- auto.commit.interval.ms (default 5000ms (5초))
주기적으로 offset을 커밋하는 시간 
- auto.offset.reset (default latest)
earliest: 가장 초기의 offset값으로 설정
latest: 가장 마지막의 offset값으로 설정
none: 이전 offset값을 찾지 못하면 error 발생